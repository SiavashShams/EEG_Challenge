{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 22:48:03.213597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 22:48:03.213667: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 22:48:03.213689: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 22:48:03.222791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample code to generate labels for test dataset of\n",
    "match-mismatch task. The requested format for submitting the labels is\n",
    "as follows:\n",
    "for each subject a json file containing a python dictionary in the\n",
    "format of  ==> {'sample_id': prediction, ... }.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "HPARAM_FILE = 'hparams/baseline/twin_tcn.yaml'\n",
    "# save/baseline/.../1234\n",
    "CKPT_PATH = 'save/baseline/env_tcn_bs64_lr1e-3_ep10/1234'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 22:48:08.325363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43412 MB memory:  -> device: 0, name: NVIDIA L40, pci bus id: 0000:61:00.0, compute capability: 8.9\n",
      "Counting batches in TF dataset: 7975it [00:26, 298.29it/s]\n",
      "Counting batches in TF dataset: 845it [00:04, 177.35it/s]\n"
     ]
    }
   ],
   "source": [
    "### Initialize and load models\n",
    "\n",
    "# Overwrite hparams\n",
    "argv = [HPARAM_FILE, '--time_stamp', 'XXX']\n",
    "argv += ['--use_wandb', 'false']\n",
    "argv += ['--n_mismatch', '4']\n",
    "argv += ['--batch_size', '64']\n",
    "argv += ['--batch_equalizer', 'all']\n",
    "argv += ['--save_folder', CKPT_PATH]\n",
    "\n",
    "hparam_file, run_opts, overrides = sb.parse_arguments(argv)\n",
    "\n",
    "with open(HPARAM_FILE) as f:\n",
    "    hparams = load_hyperpyyaml(f, overrides)\n",
    "    \n",
    "run_opts['auto_mix_prec'] = hparams['mix_prec'] # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 845/845 [00:18<00:00, 45.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch None:  Stage.TEST {'valid_loss': 1.1962, 'valid_acc': 0.5246, 'epoch': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1961566411531896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SANITY CHECK: Evaluate the model on valid set\n",
    "# to make sure the weights are properly loaded\n",
    "\n",
    "class Classifier(sb.core.Brain):\n",
    "\n",
    "    def compute_forward(self, eeg, speech, stage):\n",
    "        eeg_emb = self.modules.eeg_model(eeg)\n",
    "        speech_emb = self.modules.speech_model(speech)\n",
    "        pred = self.modules.classifier(eeg_emb, speech_emb)\n",
    "                \n",
    "        return pred\n",
    "    \n",
    "    def compute_objectives(self, pred, label, stage):\n",
    "        B = label.shape[0]\n",
    "        \n",
    "        loss = self.hparams.loss_fn(pred, label)\n",
    "        est_label = torch.argmax(pred, dim=-1)\n",
    "        acc = sum(est_label==label) / B\n",
    "        \n",
    "        self.loss_stat['loss'] += float(loss) * B\n",
    "        self.loss_stat['acc'] += float(acc) * B\n",
    "        self.count += B\n",
    "    \n",
    "        return loss\n",
    "\n",
    "    def make_dataloader(\n",
    "        self, dataset, stage, ckpt_prefix=\"dataloader-\", **loader_kwargs\n",
    "    ):\n",
    "        # Treat pytorch TF wrapper as a dataloader\n",
    "        # Because create_tf_dataset already batches EEGs and speeches\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        eeg, speech, label = batch\n",
    "        \n",
    "        eeg = eeg.to(self.device) # (B*5, 64, 320)\n",
    "        speech = speech.to(self.device) # (B*5, 5, 1, 320)\n",
    "        label = label.to(self.device)\n",
    "        \n",
    "        # Forward\n",
    "        with torch.no_grad():\n",
    "            pred = self.compute_forward(eeg, speech, stage)\n",
    "            loss = self.compute_objectives(pred, label, stage)\n",
    "            \n",
    "        return loss.detach().cpu()\n",
    "        \n",
    "        \n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        super().on_stage_start(stage, epoch)\n",
    "        self.count = 0\n",
    "        self.loss_stat = {\n",
    "            'loss': 0,\n",
    "            'acc': 0,\n",
    "        }\n",
    "        # Reload windows at the start of each epoch\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.hparams.train_windows.reload()\n",
    "        elif stage == sb.Stage.VALID:\n",
    "            self.hparams.valid_windows.reload()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        for loss_key in self.loss_stat:\n",
    "            self.loss_stat[loss_key] /= self.count\n",
    "        \n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            stage_stats = {'train_'+key: round(float(value), 4) for key, value in self.loss_stat.items()}\n",
    "            stage_stats['lr'] = self.optimizer.param_groups[0]['lr']\n",
    "            stage_stats['epoch'] = epoch\n",
    "    \n",
    "        elif stage == sb.Stage.VALID:\n",
    "            stage_stats = {'valid_'+key: round(float(value), 4) for key, value in self.loss_stat.items()}\n",
    "            stage_stats['epoch'] = epoch\n",
    "            self.lr_scheduler.step(self.loss_stat['loss'])\n",
    "            \n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=self.loss_stat, max_keys=['acc'],\n",
    "            )\n",
    "            \n",
    "        elif stage == sb.Stage.TEST:\n",
    "            stage_stats = {'valid_'+key: round(float(value), 4) for key, value in self.loss_stat.items()}\n",
    "            stage_stats['epoch'] = epoch\n",
    "                        \n",
    "            assert stage_stats['valid_acc'] > 0.5\n",
    "            print(f'Epoch {epoch}: ', stage, stage_stats)\n",
    "        \n",
    "brain = Classifier(\n",
    "    modules=hparams['modules'],\n",
    "    opt_class=hparams['optimizer'],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams['checkpointer'],\n",
    ")\n",
    "\n",
    "brain.evaluate(\n",
    "    test_set=hparams['valid_windows'],\n",
    "    max_key='acc'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['eeg_model'].eval()\n",
    "hparams['speech_model'].eval()\n",
    "hparams['classifier'].eval()\n",
    "\n",
    "def predict(eeg, speech):\n",
    "    \n",
    "    eeg = eeg.squeeze(0).permute(0, 2, 1).contiguous() # (437, 64, 320)\n",
    "    speech = speech.permute(1, 0, 3, 2).contiguous() # (437, 5, 1, 320)\n",
    "    \n",
    "    eeg_emb = hparams['eeg_model'](eeg)\n",
    "    speech_emb = hparams['speech_model'](speech)\n",
    "    pred = hparams['classifier'](eeg_emb, speech_emb)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start baseline evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42191/1051499654.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  data_eeg = torch.tensor(data_eeg).float().cuda() # (1, 437, 320, 64)\n"
     ]
    }
   ],
   "source": [
    "### Evaluation starts\n",
    "print('Start baseline evaluation...')\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# Length of the decision window\n",
    "window_length_s = 5\n",
    "fs = 64\n",
    "\n",
    "window_length = window_length_s * fs  # 5 seconds\n",
    "number_mismatch = 4 \n",
    "\n",
    "# Provide the path of the dataset\n",
    "data_folder = '/engram/naplab/shared/eeg_challenge_data_test/homes.esat.kuleuven.be/~lbollens/sparrkulee/test_set/TASK1_match_mismatch'\n",
    "eeg_folder = os.path.join(data_folder, 'preprocessed_eeg')\n",
    "stimulus_folder = os.path.join(data_folder, 'stimulus')\n",
    "\n",
    "# # stimulus feature which will be used for training the model. Can be either 'envelope' ( dimension 1) or 'mel' (dimension 28)\n",
    "stimulus_features = [\"envelope\"]\n",
    "stimulus_dimension = 1\n",
    "\n",
    "features = [\"eeg\"] + stimulus_features\n",
    "\n",
    "test_eeg_mapping = glob.glob(os.path.join(data_folder, 'sub*mapping.json'))\n",
    "\n",
    "test_stimuli = glob.glob(os.path.join(stimulus_folder, f'*{stimulus_features[0]}*chunks.npz'))\n",
    "\n",
    "#load all test stimuli\n",
    "test_stimuli_data = {}\n",
    "for stimulus_path in test_stimuli:\n",
    "    test_stimuli_data = dict(test_stimuli_data, **np.load(stimulus_path))\n",
    "\n",
    "for sub_stimulus_mapping in test_eeg_mapping:\n",
    "    subject = os.path.basename(sub_stimulus_mapping).split('_')[0]\n",
    "\n",
    "    # load stimulus mapping\n",
    "    sub_stimulus_mapping = json.load(open(sub_stimulus_mapping))\n",
    "\n",
    "    #load eeg data\n",
    "    sub_path = os.path.join(eeg_folder, f'{subject}_eeg.npz')\n",
    "    sub_eeg_data = dict(np.load(sub_path))\n",
    "\n",
    "\n",
    "    data_eeg =  np.stack([[sub_eeg_data[value['eeg']]]  for key, value in sub_stimulus_mapping.items() ])\n",
    "    # change dim 0 and 1 of eeg and unstack\n",
    "    data_eeg = np.swapaxes(data_eeg, 0, 1)\n",
    "    data_eeg = list(data_eeg)\n",
    "\n",
    "    data_stimuli = np.stack([[test_stimuli_data[x] for x in value['stimulus']] for key, value in sub_stimulus_mapping.items()])\n",
    "    # change dim 0 and 1 of stimulus and unstack\n",
    "    data_stimuli = np.swapaxes(data_stimuli, 0, 1)\n",
    "    data_stimuli = list(data_stimuli)\n",
    "    id_list= list(sub_stimulus_mapping.keys()) # 437\n",
    "\n",
    "    ### Call pytorch model. Other TF stuff unchanged.\n",
    "    with torch.inference_mode():\n",
    "        data_eeg = torch.tensor(data_eeg).float().cuda() # (1, 437, 320, 64)\n",
    "        data_stimuli = torch.tensor(data_stimuli).float().cuda() # (5, 437, 320, 1)\n",
    "        preds = predict(data_eeg, data_stimuli).cpu().numpy()\n",
    "    \n",
    "    # predictions = model.predict(data_eeg + data_stimuli)\n",
    "    # predictions = predict\n",
    "    \n",
    "    labels = np.argmax(preds, axis=1)\n",
    "\n",
    "    sub = dict(zip(id_list, [int(x) for x in labels]))\n",
    "\n",
    "    prediction_dir = 'predictions'\n",
    "    os.makedirs(prediction_dir, exist_ok=True)\n",
    "    with open(os.path.join(prediction_dir, subject + '.json'), 'w') as f:\n",
    "        json.dump(sub, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-EEGCh24]",
   "language": "python",
   "name": "conda-env-.conda-EEGCh24-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
